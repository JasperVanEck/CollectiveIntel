{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmgCUk4WFleR"
   },
   "source": [
    "# Groeps Opdracht CI\n",
    "## Joost Vledder, Sadjia Safdari, Simon Kreulen & Jasper van Eck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inhoudsopgave <a name='Top'></a>\n",
    "\n",
    "[Imports](#import)\n",
    "\n",
    "[Load data](#loaddata)\n",
    "\n",
    "[Content Based Filtering Algorithm](#contentbased)\n",
    "\n",
    "[Data Clean Up](#cleanup)\n",
    "\n",
    "[Basic Data Stats](#basicdata)\n",
    "\n",
    "[Plots](#plots)\n",
    "\n",
    "[Cosine Similiraty Content Based](#cossim)\n",
    "\n",
    "[Query vector](#queryvector)\n",
    "\n",
    "[SVD](#svd)\n",
    "\n",
    "[K-Means](#kmeans)\n",
    "\n",
    "[Cohen Kappa](#cohenkappa)\n",
    "\n",
    "[Item-Based Collaborative Filtering Algorithm](#itembasedcollab)\n",
    "\n",
    "[Basic Data Stats for Collabaritive Filtering](#basicdatacollab)\n",
    "\n",
    "[Implementation](#implementation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "giTmkchgFleV"
   },
   "source": [
    "### Imports <a name='import'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7im3ZUTtFleY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.seterr(divide='raise', over='raise', under='raise', invalid='raise')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import json\n",
    "#pip install geopy\n",
    "from geopy import geocoders\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1sKruAeFlep"
   },
   "source": [
    "### Load data <a name='loaddata'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "th5wJc8yFler",
    "outputId": "e6ee5c01-4d29-419a-9f08-6596ad923a57"
   },
   "outputs": [],
   "source": [
    "rootdir = './yelp/data'\n",
    "# rootdir = './yelp/yelp/data'\n",
    "\n",
    "df_business = pd.DataFrame()\n",
    "df_users = pd.DataFrame()\n",
    "df_reviews = pd.DataFrame()\n",
    "\n",
    "count = 0\n",
    "\n",
    "def load_jsons(data_path, file):\n",
    "    # function to help load json files, since sometimes they give utf8 encoding errors, sometimes they don't\n",
    "    file_path = os.path.join(subdir, file)\n",
    "    lines = []\n",
    "    with open(file_path) as jsons:\n",
    "        try:\n",
    "            lines = [json.loads(json_line) for json_line in jsons]\n",
    "        except:\n",
    "            print(file_path)\n",
    "    return pd.DataFrame(lines)\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    if count == 200: #1078 is the total amount of cities\n",
    "        break\n",
    "        \n",
    "    for file in files:\n",
    "        if os.path.basename(file) == 'business.json':\n",
    "            df_business = df_business.append(load_jsons(subdir, file))\n",
    "        if os.path.basename(file) == 'user.json':\n",
    "            df_users = df_users.append(load_jsons(subdir, file))\n",
    "        if os.path.basename(file) == 'review.json':\n",
    "            df_reviews = df_reviews.append(load_jsons(subdir, file))\n",
    "    count += 1\n",
    "\n",
    "df_business = df_business.set_index('business_id')\n",
    "df_users = df_users.set_index('user_id')\n",
    "df_reviews = df_reviews.set_index('review_id')\n",
    "\n",
    "display(df_business.head())\n",
    "display(df_users.head())\n",
    "display(df_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3P5IHlh_Fle2"
   },
   "outputs": [],
   "source": [
    "df_business.to_pickle('./business_pickle.pkl')\n",
    "df_users.to_pickle('./users_pickle.pkl')\n",
    "df_reviews.to_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8s5fYo_Fle_"
   },
   "outputs": [],
   "source": [
    "df_business = pd.read_pickle('./business_pickle.pkl')\n",
    "df_users = pd.read_pickle('./users_pickle.pkl')\n",
    "df_reviews = pd.read_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_business)\n",
    "display(df_users)\n",
    "display(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILjMqUbwFlfG"
   },
   "source": [
    "### Clean up Data For Content Based <a name='cleanup'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQAexx_mFlfH"
   },
   "outputs": [],
   "source": [
    "cat_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Drop na categories\n",
    "df_business = df_business[df_business['categories'].notna()]\n",
    "\n",
    "# Normalize long, lat, stars & review_count\n",
    "#x = cos(lat) * cos(lon)\n",
    "#y = cos(lat) * sin(lon)\n",
    "df_business['stars'] = (df_business['stars']-df_business['stars'].min()) / (df_business['stars'].max()-df_business['stars'].min())\n",
    "df_business['review_count'] = (df_business['review_count']-df_business['review_count'].min()) / (df_business['review_count'].max()-df_business['review_count'].min())\n",
    "#df_business['x_axis'] = np.cos(df_business['latitude']) * np.cos(df_business['longitude'])\n",
    "#df_business['y_axis'] = np.cos(df_business['latitude']) * np.sin(df_business['longitude'])\n",
    "\n",
    "# Dataframe of categories one hot encoded per business_id\n",
    "df_tmp = df_business['categories'].str.split(pat=', ',expand=True)\n",
    "\n",
    "# Create dict of categories\n",
    "for index, row in df_tmp.iterrows():\n",
    "    for _,elem in row.items():\n",
    "        if elem and elem not in cat_dict[index]:\n",
    "            cat_dict[index][elem] += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhwRJvKcFlfP"
   },
   "outputs": [],
   "source": [
    "# Create DF from dict\n",
    "df_cats = pd.DataFrame.from_dict(cat_dict, orient='index')\n",
    "\n",
    "#Drop NaN column\n",
    "df_cats = df_cats.drop(columns='NaN',errors='ignore')\n",
    "\n",
    "# Fillna with 0\n",
    "df_cats = df_cats.fillna(0)\n",
    "\n",
    "# Join back into business\n",
    "df_business = df_business.join(df_cats,on='business_id')\n",
    "\n",
    "# Drop cols\n",
    "df_business = df_business.drop(columns=['address','state','postal_code','attributes','categories','hours'],errors='ignore')\n",
    "\n",
    "# Remove elite & friends columns because of non use\n",
    "df_users = df_users.drop(columns=['elite','friends'],axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVdgRVODFlfV",
    "outputId": "e7bc9cbc-ae7a-490b-88cc-54824a55b9a9"
   },
   "outputs": [],
   "source": [
    "display(df_business.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1tr28qCFlfb"
   },
   "outputs": [],
   "source": [
    "df_business.to_pickle('./business_pickle.pkl')\n",
    "df_users.to_pickle('./users_pickle.pkl')\n",
    "df_reviews.to_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = pd.read_pickle('./business_pickle.pkl')\n",
    "df_users = pd.read_pickle('./users_pickle.pkl')\n",
    "df_reviews = pd.read_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFKZ1tMYFlfi"
   },
   "source": [
    "### Basic Data Stats for Content Based <a name='basicdata'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQ3QBy83Flfj",
    "outputId": "d27868b8-3425-495e-e204-b3164bc3fbd3"
   },
   "outputs": [],
   "source": [
    "# Some information about the category amounts per business\n",
    "\n",
    "# series of amount of categories per business\n",
    "cats_business = df_cats.sum(axis=1).sort_values(ascending=False)\n",
    "display(cats_business)\n",
    "\n",
    "avg_cats_bus = cats_business.mean()\n",
    "max_cats_bus = cats_business.max()\n",
    "min_cats_bus = cats_business.min()\n",
    "\n",
    "print('Average amount of categories for businesses: ', avg_cats_bus)\n",
    "print('Minimum amount of categories for businesses: ', min_cats_bus)\n",
    "print('Maximum amount of categories for businesses: ', max_cats_bus)\n",
    "\n",
    "# series of amount of businesses with the index amount of categories\n",
    "cat_bus_distribution = cats_business.value_counts()\n",
    "display(cat_bus_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oC4TPFiBFlfp",
    "outputId": "53063171-ff50-44b8-a4b7-743bf76fb084"
   },
   "outputs": [],
   "source": [
    "# series of number of businesses with index as category\n",
    "cats_presences = df_cats.sum(axis=0).sort_values(ascending=False)\n",
    "display(cats_presences)\n",
    "\n",
    "avg_cats = cats_presences.mean()\n",
    "max_cats = cats_presences.max()\n",
    "min_cats = cats_presences.min()\n",
    "\n",
    "print('Average amount of presence of a category: ', avg_cats)\n",
    "print('Minimum amount of presence of a category: ', min_cats)\n",
    "print('Maximum amount of presences of a categry: ', max_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyxJ_0yjFlfv"
   },
   "source": [
    "### Plots <a name='plots'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0J2DzcCLFlfx"
   },
   "outputs": [],
   "source": [
    "hist_bus = df_business.hist(column=['stars','review_count'])\n",
    "# df_business.plot(x='x_axis',y='y_axis',kind='scatter')\n",
    "plt.show()\n",
    "cats_presences.plot()\n",
    "plt.show()\n",
    "cats_business.plot()\n",
    "plt.show()\n",
    "\n",
    "total_cats = sum(cat_bus_distribution.values)\n",
    "percentages = [(value/total_cats) * 100 for value in cat_bus_distribution.values]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(percentages, labels=cat_bus_distribution.index, autopct='%1.1f%%')\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = pd.read_pickle('./business_pickle.pkl')\n",
    "df_cats = pd.read_pickle('./cats_pickle.pkl')\n",
    "df_users = pd.read_pickle('./users_pickle.pkl')\n",
    "df_reviews = pd.read_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity <a name='cossim'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vectors are series, convert to np array\n",
    "def cosineSim(vector, docVector):\n",
    "    vector = vector.to_numpy()\n",
    "    docVector = docVector.to_numpy()\n",
    "    \n",
    "    # Get lengths of vectors\n",
    "    sim = (np.sqrt(sum(vector**2))*np.sqrt(sum(docVector**2)))\n",
    "    \n",
    "    # Ensure no division by 0\n",
    "    if sim == 0.:\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate cosine sim\n",
    "    return vector.dot(docVector)/sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankedBusinessList(df_business, queryVector):\n",
    "    df_bus_features = df_business.copy().drop(columns=['name','city','stars','review_count','is_open','Score Cos'],errors='ignore')\n",
    "    scoreList = np.zeros(len(df_business))\n",
    "    for i in range(len(df_bus_features)):\n",
    "        scoreList[i] = cosineSim(queryVector, df_bus_features.iloc[i])\n",
    "    \n",
    "    df_business['Score Cos'] = scoreList\n",
    "    return df_business.sort_values(by=['Score Cos','stars'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Query Vector <a name='queryvector'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first cell get the longitude and latitude of a given city and state, and normalizes both values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_business['city'].value_counts().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_string = 'Scottsdale AZ'\n",
    "gn = Nominatim(user_agent='WalterGKurtz')\n",
    "location = gn.geocode(loc_string)\n",
    "queryVector = pd.Series(data=0.,index=df_business.columns)\n",
    "queryVector.at['latitude'] = location.latitude\n",
    "queryVector.at['longitude'] = location.longitude\n",
    "#x = cos(lat) * cos(lon)\n",
    "#y = cos(lat) * sin(lon)\n",
    "#queryVector.at['x_axis'] = np.cos(location.latitude) * np.cos(location.longitude)\n",
    "#queryVector.at['y_axis'] = np.cos(location.latitude) * np.sin(location.longitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second cell inputs the preferences of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preferences\n",
    "queryVector = queryVector.drop(labels=['name','city','stars','review_count','is_open'],errors='ignore')\n",
    "queryVector.at['Automotive'] = 1.\n",
    "queryVector.at['Auto Repair'] = 1.\n",
    "queryVector.at['Fast Food'] = 1.\n",
    "queryVector.at['Restaurants'] = 1.\n",
    "queryVector.at['Pizza'] = 1.\n",
    "queryVector.at['Men\\'s Clothing'] = 1.\n",
    "queryVector.at['Women\\'s Clothing'] = 1.\n",
    "queryVector.at['Fashion'] = 1.\n",
<<<<<<< HEAD
    "queryVector.at['Bars'] = 1.\n",
=======
    "queryVector.at['Bars'] = 0\n",
>>>>>>> 8d060bc66e7e5b3376c00b6386afb3ba3f1d28b9
    "display(queryVector)\n",
    "#queryVector.to_csv('test.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rankedBusinessList(df_business,queryVector).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD <a name='svd'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = df_business['stars'].values\n",
    "cities = df_business['city'].values\n",
    "names = df_business['name'].values\n",
    "bus_ids = df_business.index\n",
    "df_tmp_svd = df_business.copy().drop(columns=['name','city', 'stars','review_count','is_open', 'Score Cos'],errors='ignore').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code can be used to find the optimal amount of dimension by the SVD\n",
    "\n",
    "# trunc_svd = TruncatedSVD(n_components=len(df_tmp_svd[0])-1).fit(df_tmp_svd)\n",
    "# reduced_x = trunc_svd.transform(df_tmp_svd)\n",
    "# singles = trunc_svd.singular_values_\n",
    "\n",
    "# eigvals = singles**2 / np.sum(singles**2)\n",
    "# fig = plt.figure(figsize=(8,5))\n",
    "# sing_vals = np.arange(len(df_tmp_svd[0])-1) + 1\n",
    "# plt.plot(sing_vals[2:200], eigvals[2:200], 'ro-', linewidth=2)\n",
    "# plt.title('Scree Plot')\n",
    "# plt.xlabel('Component number')\n",
    "# plt.ylabel('Eigenvalue')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dims = 25\n",
    "trunc_svd = TruncatedSVD(n_components=optimal_dims).fit(df_tmp_svd)\n",
    "reduced_x = trunc_svd.transform(df_tmp_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryVectorReduced = trunc_svd.transform(np.array([queryVector]))\n",
    "\n",
    "df_reduced_x = pd.DataFrame(data=reduced_x, index=bus_ids)\n",
    "df_reduced_x.insert(0, 'stars', stars)\n",
    "df_reduced_x.insert(0, 'city', cities)\n",
    "df_reduced_x.insert(0, 'name', names)\n",
    "df_queryVectorReduced = pd.Series(data=queryVectorReduced[0])\n",
    "display(rankedBusinessList(df_reduced_x,df_queryVectorReduced).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means <a name='kmeans'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This block of code was used to find the optimal K, it takes very long to run, so that's why it is commented now.\n",
    "\n",
    "# from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# df_bus_kmeans = df_reduced_x.copy().drop(columns=['name','city','stars','Score Cos'],errors='ignore').to_numpy()\n",
    "\n",
    "# # choose k\n",
    "\n",
    "# model = KMeans(random_state=0)\n",
    "# visualizer = KElbowVisualizer(model, k=[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
    "# visualizer.fit(df_bus_kmeans)        # Fit the data to the visualizer\n",
    "# visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bus_kmeans = df_reduced_x.copy().drop(columns=['name','city','stars','Score Cos'],errors='ignore').to_numpy()\n",
    "kmeans = KMeans(n_clusters=300,random_state=0).fit(df_bus_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict query vector\n",
    "labels = np.array(kmeans.predict(df_bus_kmeans))\n",
    "prediction = kmeans.predict([df_queryVectorReduced.to_numpy()])\n",
    "\n",
    "matches = [y[0] for y,val in np.ndenumerate(labels) if val == prediction]\n",
    "\n",
    "df_matched = df_business.iloc[matches]\n",
    "\n",
    "display(df_matched.sort_values(by=['stars'],ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa <a name='cohenkappa'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataformat\n",
    "# 1 | 0\n",
    "# 1 | 1\n",
    "# 0 | 1\n",
    "\n",
    "def PE(data):\n",
    "    '''On input data, return the P(E) (expected agreement).'''\n",
    "    relevant = 0\n",
    "    nonrelevant = 0\n",
    "    # Iterate over the data\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            \n",
    "            # Top up the relevant documents by one if 1 is encountered\n",
    "            if j == 1:\n",
    "                relevant += 1\n",
    "            # Top up the nonrelevant documents by one if 0 is encountered\n",
    "            if j == 0:\n",
    "                nonrelevant += 1\n",
    "\n",
    "    # Calculates the total of inspected documents for the judges combined\n",
    "    total = len(data)*2\n",
    "\n",
    "    # Calculates the pooled marginals\n",
    "    rel = relevant/total\n",
    "    nonrel = nonrelevant/total\n",
    "\n",
    "    # Calculates the P(E)\n",
    "    P_E = nonrel**2 + rel **2    \n",
    "    return    P_E \n",
    "\n",
    "\n",
    "def kappa(data, P_E):\n",
    "    agree = 0\n",
    "    for i in data:\n",
    "        temp = None\n",
    "        for j in i:\n",
    "            if temp == j:\n",
    "                agree += 1\n",
    "            temp = j\n",
    "    P_A = agree / len(data)\n",
    "    if P_E == 1:\n",
    "        kappa = 1\n",
    "    else:\n",
    "        kappa = (P_A - P_E)/(1 - P_E)   \n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AveragePrecision(ranked_list_of_results, list_of_relevant_objects):\n",
    "    total = len(list_of_relevant_objects)\n",
    "    sumPk = 0\n",
    "    rank = 0\n",
    "    relevant = 0\n",
    "    for result in ranked_list_of_results:\n",
    "        rank += 1\n",
    "        if result in list_of_relevant_objects:\n",
    "            relevant += 1\n",
    "            sumPk += relevant/rank\n",
    "            \n",
    "    aprecision = sumPk/total\n",
    "    return aprecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_result_list = list(range(20))\n",
    "relevant_objects_cos = [0,1,2,3,4,5,6,7,8,10,11,12,19]\n",
    "relevant_objects_kmeans = [0,1,4,5,6,7,8,9,10,11,15,16,17,18]\n",
    "print('Average Precision of cosine sim: ', AveragePrecision(ranked_result_list,relevant_objects_cos))\n",
    "print('Average Precision of K-Means: ', AveragePrecision(ranked_result_list,relevant_objects_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-Based Collaborative Filtering Algorithm <a name='itembasedcollab'></a>\n",
    "\n",
    "[Top](#Top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Stats for Collaborative Based <a name='basicdatacollab'></a>\n",
    "\n",
    "[Top](#Top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_reviews = df_reviews.reset_index()\n",
    "\n",
    "# count reviews per user and reviews per business\n",
    "reviews_per_user = df_reviews.groupby(\"user_id\")[\"review_id\"].count().sort_values(ascending=False).to_frame()\n",
    "reviews_per_business = df_reviews.groupby(\"business_id\")[\"review_id\"].count().sort_values(ascending=False).to_frame()\n",
    "\n",
    "\n",
    "# plot both review counts\n",
    "user_plot = reviews_per_user.plot()\n",
    "business_plot = reviews_per_business.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "userplot --> the distribution of the reviews over the users (long-tail): considering the fact that there are more\n",
    "reviews on business than users (one user can rate more than 1 business), it is probably better to choose\n",
    "item-based CF than user-based CF. \n",
    "\n",
    "\"\"\"\"\"\n",
    "users_series = df_users['review_count'].sort_values(ascending=False).drop_duplicates(keep='first')\n",
    "\n",
    "display(users_series)\n",
    "userplot = users_series.plot()\n",
    "\n",
    "userplot.set(xlabel='user_id', ylabel='review_count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation <a name='implementation'></a>\n",
    "\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df_reviews[:5000]\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_ratings(ratings):\n",
    "    pivottable = ratings.pivot_table(index=\"business_id\", columns=\"user_id\", values=\"stars\")\n",
    "    return pivottable\n",
    "\n",
    "utility_matrix = pivot_ratings(reviews)\n",
    "display(utility_matrix.loc['-YGQwikbX2fXUIjyegR7pw'][utility_matrix.loc['-YGQwikbX2fXUIjyegR7pw'].notna()])\n",
    "display(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(matrix, id1, id2):\n",
    "    selected_features = matrix.loc[id1].notna() & matrix.loc[id2].notna()\n",
    "    \n",
    "    # if no matching features, return 'not a number' (NaN)\n",
    "    if not selected_features.any():\n",
    "        return np.nan\n",
    "    \n",
    "    # get the features from the matrix\n",
    "    features1 = matrix.loc[id1][selected_features]\n",
    "    features2 = matrix.loc[id2][selected_features]\n",
    "\n",
    "    #if sum(features1)==0. or sum(features2)==0.:\n",
    "        #return 1.\n",
    "    \n",
    "    if id1 == id2:\n",
    "        return 1.\n",
    "    \n",
    "    sim = (np.sqrt(sum(features1**2))*np.sqrt(sum(features2**2)))\n",
    "    \n",
    "    if sim == 0.:\n",
    "        return np.nan\n",
    "    \n",
    "    return sum(features1*features2)/sim\n",
    "    \n",
    "def create_similarity_matrix_cosine(matrix):\n",
    "    similarity_matrix = pd.DataFrame(0, index=matrix.index, columns=matrix.index, dtype=float)\n",
    "    # Iter of rows & columns, cause I couldnt get apply/-map to work\n",
    "    for index, row in similarity_matrix.iteritems():\n",
    "        for i in row.index:\n",
    "            similarity_matrix.at[index,i] = cosine_distance(matrix,index,i)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "#display(create_similarity_matrix_euclid(matrix))\n",
    "    \n",
    "similarity = create_similarity_matrix_cosine(utility_matrix)\n",
    "display(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center_columns(matrix):\n",
    "    return matrix.apply(lambda x: x - x.mean())\n",
    "\n",
    "centered_utility_matrix = mean_center_columns(utility_matrix)\n",
    "display(centered_utility_matrix)\n",
    "\n",
    "similarity = create_similarity_matrix_cosine(centered_utility_matrix)\n",
    "display(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST neighborhood op eentje\n",
    "\n",
    "#def select_neighborhood(similarities, ratings, k):\n",
    "#    \"\"\" selects all items with similarity > 0  \"\"\"\n",
    "#    moviesNotWatched = ratings[ratings.isna()].index\n",
    "    \n",
    "#    similarities = similarities.where(lambda x : x > 0.00).dropna()\n",
    "#    similarities = similarities.drop(labels=moviesNotWatched,errors='ignore')\n",
    "    \n",
    "#    return similarities.sort_values(ascending=False)[:k]\n",
    "\n",
    "def select_neighborhood(similarities, ratings, k):\n",
    "    \"\"\" selects all items with similarity > 0  \"\"\"\n",
    "    # drop all non watched movies\n",
    "    \n",
    "    for key,v in ratings.iteritems():\n",
    "        if np.isnan(v):\n",
    "            similarities = similarities.drop(labels=key)\n",
    "    \n",
    "    for key,v in similarities.iteritems():\n",
    "        if not v > 0.:\n",
    "            similarities = similarities.drop(labels=key)\n",
    "            \n",
    "    similarities = similarities.sort_values(ascending=False)\n",
    "    \n",
    "    return similarities[:k]\n",
    "\n",
    "neighborhood = select_neighborhood(similarity[\"-YGQwikbX2fXUIjyegR7pw\"], utility_matrix[\"yzwOcVyWuXQpmDlPXWw0Mw\"], 10)\n",
    "display(neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "def weighted_mean(neighborhood, ratings):  \n",
    "    upper = 0\n",
    "    bottom = 0\n",
    "    \n",
    "    for index in neighborhood.index:\n",
    "        similarity = neighborhood.get(index)\n",
    "        upper += ratings.get(index) * similarity\n",
    "        bottom += similarity\n",
    "    \n",
    "    if bottom == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return upper / bottom\n",
    "\n",
    "\n",
    "display(sum(utility_matrix[\"-60mTTBkXsA8LwnS6gIvUQ\"]))\n",
    "print('ok')\n",
    "neighborhood1 = select_neighborhood(similarity[\"-YGQwikbX2fXUIjyegR7pw\"], utility_matrix[\"xeqfIp-zohdVKaF3ivL0Yw\"], 10)\n",
    "prediction1 = weighted_mean(neighborhood1, utility_matrix[\"-60mTTBkXsA8LwnS6gIvUQ\"])\n",
    "\n",
    "display(prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GroepsOpdracht_CI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
