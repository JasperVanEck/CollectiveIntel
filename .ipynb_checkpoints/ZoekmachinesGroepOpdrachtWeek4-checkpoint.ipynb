{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groep Opdracht Week 4 Zoekmachines\n",
    "\n",
    "## Group name: Yahoe\n",
    "\n",
    "## Students: Jasper van Eck, Ghislaine van den Boogerd, Joris Galema, Lotte Bottema\n",
    "## Student IDs: 6228194, 10996087, 11335165, 11269642\n",
    "\n",
    "### Github link: https://github.com/JasperVanEck/ZoekmachinesGroepsOpdracht\n",
    "\n",
    "### Link to our demo video: https://www.youtube.com/watch?v=R8OJBv0Kur4\n",
    "\n",
    "### Link to presentation: https://docs.google.com/presentation/d/1vsgE_xHZwQ-ztudv7EKTB0tiEP7fSZqsUIMs9K4zZE0/edit?usp=sharing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content<a name='Top'></a>\n",
    "[Import data](#ImportData)\n",
    "\n",
    "[Create the TF Dict](#TFDict)\n",
    "\n",
    "[Create the TF-IDF and Normalize](#TFIDFNorm)\n",
    "\n",
    "[Vectorize Query](#InputQuery)\n",
    "\n",
    "[Results](#Results)\n",
    "\n",
    "- [WordCloud](#WordCloud) Requirement 3\n",
    "- [Interact with Filters](#Filters) Requirements 1, 2, 4 and 5\n",
    "\n",
    "[Cohen's Kappa](#Cohen) Requirement 6\n",
    "\n",
    "[Graph of Timestamps Hits](#Graph)\n",
    "\n",
    "[Extra Information](#Extra)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data<a name='ImportData'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "import nltk\n",
    "import PIL\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "import operator\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#Open & read JSON file\n",
    "#Init empty list for json data to be stored\n",
    "jsonDataReviews = []\n",
    "with open('IMDB_reviews.json') as json_file:\n",
    "    #Loop through lines in json file, each review is on seperate line\n",
    "    for line in json_file:\n",
    "        #Append to the list of json data\n",
    "        jsonDataReviews.append(json.loads(line))\n",
    "\n",
    "#Read the data from the json file\n",
    "dataReviews = pd.DataFrame(jsonDataReviews)\n",
    "\n",
    "#Add Review_id column\n",
    "#Create index range\n",
    "review_id = list(range(len(dataReviews)))\n",
    "#Insert the index range into the DF\n",
    "dataReviews.insert(0,'review_id',review_id,True)\n",
    "#Cast to string from obj\n",
    "dataReviews['review_summary'] = dataReviews['review_summary'].astype(str)\n",
    "dataReviews['review_text'] = dataReviews['review_text'].astype(str)\n",
    "#Cast to int from str\n",
    "dataReviews['rating'] = dataReviews['rating'].astype(int)\n",
    "#Cast to bool from obj\n",
    "dataReviews['is_spoiler'] = dataReviews['is_spoiler'].astype(bool)\n",
    "#Create datetime objects from the review_date string\n",
    "dataReviews['review_date'] = [datetime.strptime(dateString, '%d %B %Y') for dateString in dataReviews['review_date'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open & read TSV file with movie details\n",
    "dataMovies = pd.read_csv('data.tsv', sep='\\t', header=0, dtype={'tconst':str,'titleType':str,\n",
    "                                                                'primaryTitle':str,'OriginalTitle':str,\n",
    "                                                                'isAdult':str,'startYear':str,'endYear':str,\n",
    "                                                                'runtimeMinutes':str,'genres':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieTitles = dataMovies[dataMovies['tconst'].isin(dataReviews['movie_id'].values)]\n",
    "movieTitles.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the movie_id with the movie name\n",
    "movieTitlesInsertList = [movieTitles[movieTitles['tconst']==movie_id]['primaryTitle'].values[0] for movie_id in dataReviews['movie_id'].values]\n",
    "dataReviews.insert(7, 'movie_title', movieTitlesInsertList, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of data\n",
    "dataReviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the TF Dict<a name='TFDict'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init a default dict\n",
    "tfDict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "#Init Porter Stemmer\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "#Use less reviews to reduce runtimes for testing/practice\n",
    "dataReviewsLess = dataReviews.head(50000).copy()\n",
    "\n",
    "#Retrieve the actual reviews\n",
    "reviewTexts = dataReviewsLess['review_text'].values\n",
    "\n",
    "#Loop through reviews\n",
    "for i in range(len(reviewTexts)):\n",
    "    #Tokenize reviews and lowercase the text\n",
    "    line = re.split('\\W+',reviewTexts[i].lower())\n",
    "    #Loop through tokens in review\n",
    "    for word in line:\n",
    "        #Stem token\n",
    "        stem = ps.stem(word)\n",
    "        #Increment frequency\n",
    "        tfDict[stem][i] += 1\n",
    "\n",
    "#Add in Corpus Frequency, Document Frequency and reposition the frequencies per document\n",
    "tfDictXtra = defaultdict(lambda: defaultdict(int))\n",
    "for word in tfDict:\n",
    "    tfDictXtra[word]['CorpusFreq'] = sum(tfDict[word].values())\n",
    "    tfDictXtra[word]['DocFreq'] = len(tfDict[word])\n",
    "    tfDictXtra[word]['Freq_per_doc'] = tfDict[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the TF-IDF and Normalize<a name='TFIDFNorm'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the total number of reviews/documents\n",
    "totalDocs = len(dataReviewsLess)\n",
    "\n",
    "#Total unique words\n",
    "totalUniqueWords = len(tfDictXtra)\n",
    "\n",
    "#Create np matrix with zeros\n",
    "tfIdf = np.zeros((totalUniqueWords,totalDocs))\n",
    "\n",
    "#Create dataframe of words with index list to get the word position in matrix for future reference\n",
    "wordsIndex = pd.DataFrame(list(tfDictXtra.keys()),columns=['Words'])\n",
    "#Create index range\n",
    "wordID = list(range(totalUniqueWords))\n",
    "#Insert the index range\n",
    "wordsIndex.insert(0,'Index',wordID,True)\n",
    "#Index counter, to keep track of location in word list\n",
    "wordCounter = 0\n",
    "\n",
    "\n",
    "#loop through words in dict\n",
    "for word in tfDictXtra:\n",
    "    #Loop through frequencies of word in a doc from dict; LET OP deze regel geeft soms AttributeError: 'int' object has no attribute 'keys'\n",
    "    #run de vorige cellen dan weer even opnieuw. Dat verhelpt t meestal\n",
    "    dictLoop = list(tfDictXtra[word]['Freq_per_doc'].keys())\n",
    "    for doc in dictLoop:\n",
    "        #Calculate the TF-IDF\n",
    "        tfIdf[wordCounter,doc] = tfDictXtra[word]['Freq_per_doc'][doc]*math.log((totalDocs/(1+tfDictXtra[word]['DocFreq'])))\n",
    "    wordCounter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transpose the tfIdf matrix and normalize, since the normalize works on rows, and we need to normalize the columns\n",
    "tfIdfNorm = preprocessing.normalize(tfIdf.T, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize query<a name='InputQuery'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting/test query\n",
    "query = input('Enter your query:' )\n",
    "#query = \"kid friendly movie\"\n",
    "\n",
    "#Create a normalized vector of query\n",
    "def vectorizeQuery(query):\n",
    "    #Create empty base vector for Term Freq\n",
    "    queryVector = np.zeros(totalUniqueWords)\n",
    "    #Tokenize and make lowercase\n",
    "    line = re.split('\\W+',query.lower())\n",
    "    #Loop through words\n",
    "    for word in line:\n",
    "        #Stem each word\n",
    "        stem = ps.stem(word)\n",
    "        #Increase term freq of query term\n",
    "        queryVector[wordsIndex[wordsIndex['Words']==stem]['Index'].values] += 1\n",
    "    \n",
    "    #Create empty base vector for TF-IDF\n",
    "    queryVectorTfIdf = np.zeros(totalUniqueWords)\n",
    "    #Loop through TF vector of query\n",
    "    for i in range(len(queryVector)):\n",
    "        #Act where a term frequency was recorded\n",
    "        if queryVector[i] != 0:\n",
    "            #Determine the which word it was based on the index\n",
    "            word = str(wordsIndex[wordsIndex['Index']==i]['Words'].values)\n",
    "            #Calculate the TF-IDF\n",
    "            queryVectorTfIdf[i] = queryVector[i]*math.log((totalDocs/(1+tfDictXtra[word]['DocFreq'])))\n",
    "    \n",
    "    #Make the TF-IDF vector a unit vector\n",
    "    length = np.sqrt(queryVectorTfIdf.dot(queryVectorTfIdf))\n",
    "    queryVectorNorm = queryVectorTfIdf/length\n",
    "    \n",
    "    #Return the unit vector\n",
    "    return queryVectorNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity matching\n",
    "def cosineSim(vector, docVector):\n",
    "    #Only dot product needed since vectors are already unit vectors and therefore the lengths are 1\n",
    "    return vector.dot(docVector)#/(length vector * length docVector)\n",
    "    \n",
    "def rankedList(queryVector):\n",
    "    #Create empty score list\n",
    "    scoreList = np.zeros(totalDocs)\n",
    "    #Loop through each doc\n",
    "    for i in range(len(tfIdfNorm)):\n",
    "        #Calculate for each doc the cosine sim. Index of scoreList = review_id\n",
    "        scoreList[i] = cosineSim(queryVector,tfIdfNorm[i])\n",
    "    \n",
    "    #Create new data frame for ranked list based on smaller DF of data\n",
    "    rankedDocList = dataReviewsLess.copy()\n",
    "    #Insert the similarity score for each review\n",
    "    rankedDocList.insert(0,'Score',scoreList,True)\n",
    "    #Sort the review similarity based on the score and return\n",
    "    return rankedDocList.sort_values(by='Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the ranking list\n",
    "rankings = rankedList(vectorizeQuery(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results<a name='Results'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud <a name='WordCloud'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://stackoverflow.com/questions/16645799/how-to-create-a-word-cloud-from-a-corpus-in-python\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data, title = \"WordCloud of Query Results\"):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=40,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "@interact\n",
    "def showingWordcloudsOfKRanking(k=(1,50,1)):\n",
    "    show_wordcloud(rankings.head(k)['review_text'])\n",
    "    \n",
    "\n",
    "@interact\n",
    "def showingWordCloudOfOneReview(i=(1,len(dataReviewsLess),1)):\n",
    "    show_wordcloud(dataReviewsLess[dataReviewsLess['review_id']==i]['review_text'].values,'WordCloud of a review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with Filters<a name='Filters'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter on the variables created by interact widget\n",
    "def showResultsTime(start_date, end_date, AmountResults, AtleastRating, spoiler, movie_title):\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "    if movie_title == 'None':\n",
    "        if spoiler == 'Both':\n",
    "            return rankings[(rankings.review_date > start_date) \n",
    "                        & (rankings.review_date < end_date) \n",
    "                        & (rankings.rating >= AtleastRating)].head(AmountResults)\n",
    "        elif spoiler == 'Yes':\n",
    "            return rankings[(rankings.review_date > start_date) \n",
    "                        & (rankings.review_date < end_date) \n",
    "                        & (rankings.rating >= AtleastRating)\n",
    "                        & (rankings.is_spoiler == True)].head(AmountResults)\n",
    "        elif spoiler == 'No':\n",
    "            return rankings[(rankings.review_date > start_date) \n",
    "                        & (rankings.review_date < end_date) \n",
    "                        & (rankings.rating >= AtleastRating)\n",
    "                        & (rankings.is_spoiler == False)].head(AmountResults)\n",
    "    else:\n",
    "        if spoiler == 'Both':\n",
    "            return rankings[(rankings.review_date > start_date) \n",
    "                        & (rankings.review_date < end_date) \n",
    "                        & (rankings.rating >= AtleastRating)\n",
    "                        & (rankings.movie_title == movie_title)].head(AmountResults)\n",
    "        elif spoiler == 'Yes':\n",
    "            return rankings[(rankings.review_date > start_date) \n",
    "                        & (rankings.review_date < end_date) \n",
    "                        & (rankings.rating >= AtleastRating)\n",
    "                        & (rankings.is_spoiler == True)\n",
    "                        & (rankings.movie_title == movie_title)].head(AmountResults)\n",
    "        elif spoiler == 'No':\n",
    "            return rankings[(rankings.review_date > start_date) \n",
    "                        & (rankings.review_date < end_date) \n",
    "                        & (rankings.rating >= AtleastRating)\n",
    "                        & (rankings.is_spoiler == False)\n",
    "                        & (rankings.movie_title == movie_title)].head(AmountResults)\n",
    "\n",
    "#Sort the movieTitles DF\n",
    "tmp = movieTitles.sort_values(by='primaryTitle')\n",
    "#Prep a list of movie titles for filter\n",
    "titles = ['None']\n",
    "titles.extend(tmp['primaryTitle'].values)\n",
    "#The interact function for faceted search\n",
    "_ = interact(showResultsTime,\n",
    "             start_date=widgets.DatePicker(value=pd.to_datetime('2014-01-01')),\n",
    "             end_date=widgets.DatePicker(value=pd.to_datetime('2019-01-01')),\n",
    "             AmountResults=(10, 100, 10),\n",
    "             AtleastRating=(1,10,1),\n",
    "             spoiler=['Both','Yes','No'],\n",
    "             movie_title=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen's Kappa<a name='Cohen'></a>\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--------------------\n",
    "COMMENTS FOR JUDGING:\n",
    "--------------------\n",
    "\n",
    "For each query, consider wether the results answer the information need descipted under the query. For examplefor query 1: Every review that is returnt about a movie where Brad Pitt acts in is correct. Reviews which mention Bradd Pitt but are about a movie where Brad Pitt does not act in are false. \n",
    "\n",
    "\n",
    "-------\n",
    "QUERIES:\n",
    "-------\n",
    "\n",
    "<topic number=\"1\"  >\n",
    "    <query>Brad Pitt movies</query>\n",
    "    <description> I want to find movies where Brad Pitt acts in.\n",
    "    </description>\n",
    "\n",
    "</topic>\n",
    "\n",
    "<topic number=\"2\"  >\n",
    "    <query>Sad tearjerking movies</query>\n",
    "    <description> I want to find all the movies which are sad and tearjerking\n",
    "    </description>\n",
    "</topic>\n",
    "\n",
    "<topic number=\"3\"  >\n",
    "    <query>Best disney movie</query>\n",
    "    <description> I'm looking for the top-rated disney movies\n",
    "    </description>\n",
    "</topic>\n",
    "\n",
    "<topic number=\"4\"  >\n",
    "    <query>Strong female lead</query>\n",
    "    <description> I'm looking for movies with strong women leads.\n",
    "    </description>\n",
    "</topic>\n",
    "\n",
    "<topic number=\"5\"  >\n",
    "    <query>Kid friendly movie</query>\n",
    "    <description> I'm looking for movies that are family friendly.\n",
    "    </description>\n",
    "</topic>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1judge = np.array([[1,1],[1,1],[1,1],[1,1],[1,1],[1,1],[1,1],[1,1],[1,1],[1,1]])\n",
    "query2judge = np.array([[1,1],[0,1],[1,0],[0,0],[0,1],[1,1],[1,1],[1,1],[0,0],[1,1]])\n",
    "query3judge = np.array([[1,1],[1,1],[1,1],[1,1],[1,1],[1,1],[0,0],[0,0],[0,0],[1,1]])\n",
    "query4judge = np.array([[1,1],[1,1],[0,0],[0,0],[0,0],[0,1],[0,0],[0,0],[0,0],[1,1]])\n",
    "query5judge = np.array([[1,1],[1,1],[0,0],[1,1],[1,1],[0,1],[1,1],[0,1],[0,0],[1,0]])\n",
    "\n",
    "def AveragePrecision(ranked_list_of_results, list_of_relevant_objects):\n",
    "    total = len(list_of_relevant_objects)\n",
    "    sumPk = 0\n",
    "    rank = 0\n",
    "    relevant = 0\n",
    "    for result in ranked_list_of_results:\n",
    "        rank += 1\n",
    "        if result in list_of_relevant_objects:\n",
    "            relevant += 1\n",
    "            sumPk += relevant/rank\n",
    "            \n",
    "    aprecision = sumPk/total\n",
    "    return aprecision\n",
    "\n",
    "#def AveragePrecision(ranked_list_of_results, list_of_relevant_objects):\n",
    "#    begin = 1/len(list_of_relevant_objects)\n",
    "#    count = 0\n",
    "#    for i, res in enumerate(ranked_list_of_results):\n",
    "#        for j, obj in enumerate(list_of_relevant_objects):\n",
    "#            if obj == res:\n",
    "#                itera = (j+1) / (i+1)\n",
    "#            count = count + itera\n",
    "#    return begin * count\n",
    "\n",
    "def PE(data):\n",
    "    '''On input data, return the P(E) (expected agreement).'''\n",
    "    relevant = 0\n",
    "    nonrelevant = 0\n",
    "    # Iterate over the data\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            \n",
    "            # Top up the relevant documents by one if 1 is encountered\n",
    "            if j == 1:\n",
    "                relevant += 1\n",
    "            # Top up the nonrelevant documents by one if 0 is encountered\n",
    "            if j == 0:\n",
    "                nonrelevant += 1\n",
    "\n",
    "    # Calculates the total of inspected documents for the judges combined\n",
    "    total = len(data)*2\n",
    "\n",
    "    # Calculates the pooled marginals\n",
    "    rel = relevant/total\n",
    "    nonrel = nonrelevant/total\n",
    "\n",
    "    # Calculates the P(E)\n",
    "    P_E = nonrel**2 + rel **2    \n",
    "    return    P_E \n",
    "\n",
    "\n",
    "def kappa(data, P_E):\n",
    "    agree = 0\n",
    "    for i in data:\n",
    "        temp = None\n",
    "        for j in i:\n",
    "            if temp == j:\n",
    "                agree += 1\n",
    "            temp = j\n",
    "    P_A = agree / len(data)\n",
    "    if P_E == 1:\n",
    "        kappa = 1\n",
    "    else:\n",
    "        kappa = (P_A - P_E)/(1 - P_E)   \n",
    "    return kappa\n",
    "\n",
    "P_EQ1 = PE(query1judge)\n",
    "P_EQ2 = PE(query2judge)\n",
    "P_EQ3 = PE(query3judge)\n",
    "P_EQ4 = PE(query4judge)\n",
    "P_EQ5 = PE(query5judge)\n",
    "\n",
    "KappaQ1 = kappa(query1judge, P_EQ1)\n",
    "KappaQ2 = kappa(query2judge, P_EQ2)\n",
    "KappaQ3 = kappa(query3judge, P_EQ3)\n",
    "KappaQ4 = kappa(query4judge, P_EQ4)\n",
    "KappaQ5 = kappa(query5judge, P_EQ5)\n",
    "\n",
    "KappaVal = [KappaQ1, KappaQ2, KappaQ3, KappaQ4, KappaQ5]\n",
    "\n",
    "plt.xlabel('Query')\n",
    "plt.ylabel('Kappa Value')\n",
    "plt.bar(np.arange(len(KappaVal)), KappaVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph of Timestamps Hits<a name='Graph'></a>\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurences(movietitles):\n",
    "    count = Counter(movietitles) \n",
    "    # sort the words from highest to lowest (first one is the highest rank)\n",
    "    sorted_count = sorted(count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #gives the list of occurences and not the words\n",
    "    occurences = []\n",
    "    for i in sorted_count:\n",
    "        occurences.append(i[1])\n",
    "    return occurences\n",
    "\n",
    "def plot_bar(lists, nameplot):\n",
    "    y = count_occurences(lists)\n",
    "    x = np.arange(len(y))\n",
    "    \n",
    "    #plots the graph of word frequency versus rank of a word in this corpus\n",
    "    plt.bar(x,y)\n",
    "    plt.xlabel(nameplot)\n",
    "    plt.ylabel('Occurancies')\n",
    "    plt.show()\n",
    "\n",
    "plt.ylim(0,250)\n",
    "plot_bar(dataReviews['review_date'], 'Date of review')\n",
    "\n",
    "plt.ylim(0,1000)\n",
    "plot_bar(dataReviews['movie_title'], 'Movie title')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra information <a name='Extra'></a>\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose for the imdb-review dataset because it is a very interesting dataset with a lot of different information. It also provided us with the possibility to use other dataset to combine even more extra information to get a full package of info about one review. \n",
    "\n",
    "Are search engine works very well, but sometimes returns \"funny\" results. For example: reviews about 'The Godfather' appear if you search for \"Kid friendly movies\". However this is logical if you look closer, because the people who wrote te review do mention that they think kids should watch it.\n",
    "\n",
    "Are wordclouds also do work very well. Same with our faceted search. Both of which execute in real-time. \n",
    "\n",
    "We did have problems implementing elastic search.\n",
    "\n",
    "Overall, we asses our quality of work to be good. We made a functioning search engine which does what we wanted it to do. It was a very meaningful and inspiring exercise and thought us all to bring the last four weeks of theory into practice. We definitly think we succeeded in making a working search engine, with more time we would have loved to have done more and expand on our current search engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A notion on Elasticsearch \n",
    "\n",
    "We do not have a sufficient implementation of Elasticsearch ready. \n",
    "Initially we had the idea that Elasticsearch was not necessary. When we did we found out that it needed more time than we thought. \n",
    "\n",
    "We had problems with storing the data in an Elastic search server and performing queries with it. The data did not seem to fit on the server even though this seemed unlikely to us. Unfortunately we did not find a solution on time. We hope that we will in the future because Elastic search does seem like a great tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
