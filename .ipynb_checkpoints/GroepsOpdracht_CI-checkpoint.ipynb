{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmgCUk4WFleR"
   },
   "source": [
    "# Groeps Opdracht CI\n",
    "## Joost Vledder, Sadjia Safdari, Simon Kreulen & Jasper van Eck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inhoudsopgave <a name='Top'></a>\n",
    "\n",
    "[Imports](#import)\n",
    "\n",
    "[Load data](#loaddata)\n",
    "\n",
    "[Content Based Filtering Algorithm](#contentbased)\n",
    "\n",
    "[Data Clean Up](#cleanup)\n",
    "\n",
    "[Basic Data Stats](#basicdata)\n",
    "\n",
    "[Plots](#plots)\n",
    "\n",
    "[Cosine Similiraty Content Based](#cossim)\n",
    "\n",
    "[Query vector](#queryvector)\n",
    "\n",
    "[SVD](#svd)\n",
    "\n",
    "[K-Means](#kmeans)\n",
    "\n",
    "[Cohen Kappa](#cohenkappa)\n",
    "\n",
    "[Item-Based Collaborative Filtering Algorithm](#itembasedcollab)\n",
    "\n",
    "[Basic Data Stats for Collabaritive Filtering](#basicdatacollab)\n",
    "\n",
    "[Implementation](#implementation)\n",
    "\n",
    "[Deel 1: Mesa](#mesa)\n",
    "\n",
    "[Deel 2: Henderson](#henderson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "giTmkchgFleV"
   },
   "source": [
    "### Imports <a name='import'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7im3ZUTtFleY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.seterr(divide='raise', over='raise', under='raise', invalid='raise')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import json\n",
    "#pip install geopy\n",
    "from geopy import geocoders\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1sKruAeFlep"
   },
   "source": [
    "### Load data <a name='loaddata'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7im3ZUTtFleP"
   },
   "outputs": [],
   "source": [
    "#rootdir = './yelp/data'\n",
    "rootdir = './yelp/data/Henderson'\n",
    "\n",
    "df_business = pd.DataFrame()\n",
    "df_users = pd.DataFrame()\n",
    "df_reviews = pd.DataFrame()\n",
    "\n",
    "count = 0\n",
    "\n",
    "def load_jsons(data_path, file):\n",
    "    # function to help load json files, since sometimes they give utf8 encoding errors, sometimes they don't\n",
    "    file_path = os.path.join(subdir, file)\n",
    "    lines = []\n",
    "    with open(file_path) as jsons:\n",
    "        try:\n",
    "            lines = [json.loads(json_line) for json_line in jsons]\n",
    "        except:\n",
    "            print(file_path)\n",
    "    return pd.DataFrame(lines)\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    if count == 200: #1078 is the total amount of cities\n",
    "        break\n",
    "        \n",
    "    for file in files:\n",
    "        if os.path.basename(file) == 'business.json':\n",
    "            df_business = df_business.append(load_jsons(subdir, file))\n",
    "        if os.path.basename(file) == 'user.json':\n",
    "            df_users = df_users.append(load_jsons(subdir, file))\n",
    "        if os.path.basename(file) == 'review.json':\n",
    "            df_reviews = df_reviews.append(load_jsons(subdir, file))\n",
    "    count += 1\n",
    "\n",
    "df_business = df_business.set_index('business_id')\n",
    "df_users = df_users.set_index('user_id')\n",
    "df_reviews = df_reviews.set_index('review_id')\n",
    "\n",
    "display(df_business.head())\n",
    "display(df_users.head())\n",
    "display(df_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3P5IHlh_Fle2"
   },
   "outputs": [],
   "source": [
    "df_business.to_pickle('./business_pickle.pkl')\n",
    "df_users.to_pickle('./users_pickle.pkl')\n",
    "df_reviews.to_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8s5fYo_Fle_"
   },
   "outputs": [],
   "source": [
    "df_business = pd.read_pickle('./business_pickle.pkl')\n",
    "df_users = pd.read_pickle('./users_pickle.pkl')\n",
    "df_reviews = pd.read_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILjMqUbwFlfG"
   },
   "source": [
    "### Clean up Data For Content Based <a name='cleanup'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQAexx_mFlfH"
   },
   "outputs": [],
   "source": [
    "cat_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Drop na categories\n",
    "df_business = df_business[df_business['categories'].notna()]\n",
    "\n",
    "# Normalize long, lat, stars & review_count\n",
    "#x = cos(lat) * cos(lon)\n",
    "#y = cos(lat) * sin(lon)\n",
    "df_business['stars'] = (df_business['stars']-df_business['stars'].min()) / (df_business['stars'].max()-df_business['stars'].min())\n",
    "df_business['review_count'] = (df_business['review_count']-df_business['review_count'].min()) / (df_business['review_count'].max()-df_business['review_count'].min())\n",
    "#df_business['x_axis'] = np.cos(df_business['latitude']) * np.cos(df_business['longitude'])\n",
    "#df_business['y_axis'] = np.cos(df_business['latitude']) * np.sin(df_business['longitude'])\n",
    "\n",
    "# Dataframe of categories one hot encoded per business_id\n",
    "df_tmp = df_business['categories'].str.split(pat=', ',expand=True)\n",
    "\n",
    "# Create dict of categories\n",
    "for index, row in df_tmp.iterrows():\n",
    "    for _,elem in row.items():\n",
    "        if elem and elem not in cat_dict[index]:\n",
    "            cat_dict[index][elem] += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhwRJvKcFlfP"
   },
   "outputs": [],
   "source": [
    "# Create DF from dict\n",
    "df_cats = pd.DataFrame.from_dict(cat_dict, orient='index')\n",
    "\n",
    "#Drop NaN column\n",
    "df_cats = df_cats.drop(columns='NaN',errors='ignore')\n",
    "\n",
    "# Fillna with 0\n",
    "df_cats = df_cats.fillna(0)\n",
    "\n",
    "# Join back into business\n",
    "df_business = df_business.join(df_cats,on='business_id')\n",
    "\n",
    "# Drop cols\n",
    "df_business = df_business.drop(columns=['address','state','postal_code','attributes','categories','hours'],errors='ignore')\n",
    "\n",
    "# Remove elite & friends columns because of non use\n",
    "df_users = df_users.drop(columns=['elite','friends'],axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1tr28qCFlfb"
   },
   "outputs": [],
   "source": [
    "df_business.to_pickle('./business_pickle.pkl')\n",
    "df_users.to_pickle('./users_pickle.pkl')\n",
    "df_reviews.to_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = pd.read_pickle('./business_pickle.pkl')\n",
    "df_users = pd.read_pickle('./users_pickle.pkl')\n",
    "df_reviews = pd.read_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFKZ1tMYFlfi"
   },
   "source": [
    "### Basic Data Stats for Content Based <a name='basicdata'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQ3QBy83Flfj"
   },
   "outputs": [],
   "source": [
    "# Some information about the category amounts per business\n",
    "\n",
    "# series of amount of categories per business\n",
    "cats_business = df_cats.sum(axis=1).sort_values(ascending=False)\n",
    "display(cats_business)\n",
    "\n",
    "avg_cats_bus = cats_business.mean()\n",
    "max_cats_bus = cats_business.max()\n",
    "min_cats_bus = cats_business.min()\n",
    "\n",
    "print('Average amount of categories for businesses: ', avg_cats_bus)\n",
    "print('Minimum amount of categories for businesses: ', min_cats_bus)\n",
    "print('Maximum amount of categories for businesses: ', max_cats_bus)\n",
    "\n",
    "# series of amount of businesses with the index amount of categories\n",
    "cat_bus_distribution = cats_business.value_counts()\n",
    "display(cat_bus_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oC4TPFiBFlfp",
    "outputId": "53063171-ff50-44b8-a4b7-743bf76fb084"
   },
   "outputs": [],
   "source": [
    "# series of number of businesses with index as category\n",
    "cats_presences = df_cats.sum(axis=0).sort_values(ascending=False)\n",
    "display(cats_presences)\n",
    "\n",
    "avg_cats = cats_presences.mean()\n",
    "max_cats = cats_presences.max()\n",
    "min_cats = cats_presences.min()\n",
    "\n",
    "print('Average amount of presence of a category: ', avg_cats)\n",
    "print('Minimum amount of presence of a category: ', min_cats)\n",
    "print('Maximum amount of presences of a category: ', max_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyxJ_0yjFlfv"
   },
   "source": [
    "### Plots <a name='plots'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0J2DzcCLFlfx"
   },
   "outputs": [],
   "source": [
    "hist_bus = df_business.hist(column=['stars','review_count'])\n",
    "#df_business.plot(x='x_axis',y='y_axis',kind='scatter')\n",
    "df_business.plot(x='longitude',y='latitude',kind='scatter')\n",
    "plt.show()\n",
    "cats_presences.plot()\n",
    "plt.show()\n",
    "\n",
    "cats_business.plot(kind='hist',subplots=False,sharex=False,sharey=False,title='Amount of businesses per amount of categories')\n",
    "plt.show()\n",
    "\n",
    "total_cats = sum(cat_bus_distribution.values)\n",
    "percentages = [(value/total_cats) * 100 for value in cat_bus_distribution.values]\n",
    "\n",
    "fig1, circle = plt.subplots()\n",
    "circle.pie(percentages, labels=cat_bus_distribution.index, autopct='%1.1f%%')\n",
    "circle.axis('equal')\n",
    "plt.title('Percentual distribution of amount of catergories per business')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = pd.read_pickle('./business_pickle.pkl')\n",
    "df_cats = pd.read_pickle('./cats_pickle.pkl')\n",
    "df_users = pd.read_pickle('./users_pickle.pkl')\n",
    "df_reviews = pd.read_pickle('./reviews_pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity <a name='cossim'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vectors are series, convert to np array\n",
    "def cosineSim(vector, docVector):\n",
    "    vector = vector.to_numpy()\n",
    "    docVector = docVector.to_numpy()\n",
    "    \n",
    "    # Get lengths of vectors\n",
    "    sim = (np.sqrt(sum(vector**2))*np.sqrt(sum(docVector**2)))\n",
    "    \n",
    "    # Ensure no division by 0\n",
    "    if sim == 0.:\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate cosine sim\n",
    "    return vector.dot(docVector)/sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankedBusinessList(df_business, queryVector):\n",
    "    df_bus_features = df_business.copy().drop(columns=['name','city','stars','review_count','is_open','Score Cos'],errors='ignore')\n",
    "    scoreList = np.zeros(len(df_business))\n",
    "    for i in range(len(df_bus_features)):\n",
    "        scoreList[i] = cosineSim(queryVector, df_bus_features.iloc[i])\n",
    "    \n",
    "    df_business['Score Cos'] = scoreList\n",
    "    return df_business.sort_values(by=['Score Cos','stars'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Query Vector <a name='queryvector'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first cell get the longitude and latitude of a given city and state, and normalizes both values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_business['city'].value_counts().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_string = 'Scottsdale AZ'\n",
    "gn = Nominatim(user_agent='WalterGKurtz')\n",
    "location = gn.geocode(loc_string)\n",
    "queryVector = pd.Series(data=0.,index=df_business.columns)\n",
    "queryVector.at['latitude'] = location.latitude\n",
    "queryVector.at['longitude'] = location.longitude\n",
    "#x = cos(lat) * cos(lon)\n",
    "#y = cos(lat) * sin(lon)\n",
    "#queryVector.at['x_axis'] = np.cos(location.latitude) * np.cos(location.longitude)\n",
    "#queryVector.at['y_axis'] = np.cos(location.latitude) * np.sin(location.longitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second cell inputs the preferences of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preferences\n",
    "queryVector = queryVector.drop(labels=['name','city','stars','review_count','is_open'],errors='ignore')\n",
    "queryVector.at['Automotive'] = 1.\n",
    "queryVector.at['Auto Repair'] = 1.\n",
    "queryVector.at['Fast Food'] = 1.\n",
    "queryVector.at['Restaurants'] = 1.\n",
    "queryVector.at['Pizza'] = 1.\n",
    "queryVector.at['Men\\'s Clothing'] = 1.\n",
    "queryVector.at['Women\\'s Clothing'] = 1.\n",
    "queryVector.at['Fashion'] = 1.\n",
    "queryVector.at['Bars'] = 1.\n",
    "display(queryVector)\n",
    "#queryVector.to_csv('test.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rankedBusinessList(df_business,queryVector).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD <a name='svd'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = df_business['stars'].values\n",
    "cities = df_business['city'].values\n",
    "names = df_business['name'].values\n",
    "bus_ids = df_business.index\n",
    "df_tmp_svd = df_business.copy().drop(columns=['name','city', 'stars','review_count','is_open', 'Score Cos'],errors='ignore').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code can be used to find the optimal amount of dimension by the SVD\n",
    "\n",
    "# trunc_svd = TruncatedSVD(n_components=len(df_tmp_svd[0])-1).fit(df_tmp_svd)\n",
    "# reduced_x = trunc_svd.transform(df_tmp_svd)\n",
    "# singles = trunc_svd.singular_values_\n",
    "\n",
    "# eigvals = singles**2 / np.sum(singles**2)\n",
    "# fig = plt.figure(figsize=(8,5))\n",
    "# sing_vals = np.arange(len(df_tmp_svd[0])-1) + 1\n",
    "# plt.plot(sing_vals[2:200], eigvals[2:200], 'ro-', linewidth=2)\n",
    "# plt.title('Scree Plot')\n",
    "# plt.xlabel('Component number')\n",
    "# plt.ylabel('Eigenvalue')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dims = 25\n",
    "trunc_svd = TruncatedSVD(n_components=optimal_dims).fit(df_tmp_svd)\n",
    "reduced_x = trunc_svd.transform(df_tmp_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryVectorReduced = trunc_svd.transform(np.array([queryVector]))\n",
    "\n",
    "df_reduced_x = pd.DataFrame(data=reduced_x, index=bus_ids)\n",
    "df_reduced_x.insert(0, 'stars', stars)\n",
    "df_reduced_x.insert(0, 'city', cities)\n",
    "df_reduced_x.insert(0, 'name', names)\n",
    "df_queryVectorReduced = pd.Series(data=queryVectorReduced[0])\n",
    "display(rankedBusinessList(df_reduced_x,df_queryVectorReduced).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means <a name='kmeans'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This block of code was used to find the optimal K, it takes very long to run, so that's why it is commented now.\n",
    "\n",
    "# from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# df_bus_kmeans = df_reduced_x.copy().drop(columns=['name','city','stars','Score Cos'],errors='ignore').to_numpy()\n",
    "\n",
    "# # choose k\n",
    "\n",
    "# model = KMeans(random_state=0)\n",
    "# visualizer = KElbowVisualizer(model, k=[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
    "# visualizer.fit(df_bus_kmeans)        # Fit the data to the visualizer\n",
    "# visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bus_kmeans = df_reduced_x.copy().drop(columns=['name','city','stars','Score Cos'],errors='ignore').to_numpy()\n",
    "kmeans = KMeans(n_clusters=300,random_state=0).fit(df_bus_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict query vector\n",
    "labels = np.array(kmeans.predict(df_bus_kmeans))\n",
    "prediction = kmeans.predict([df_queryVectorReduced.to_numpy()])\n",
    "\n",
    "matches = [y[0] for y,val in np.ndenumerate(labels) if val == prediction]\n",
    "\n",
    "df_matched = df_business.iloc[matches]\n",
    "\n",
    "display(df_matched.sort_values(by=['stars'],ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa <a name='cohenkappa'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataformat\n",
    "# 1 | 0\n",
    "# 1 | 1\n",
    "# 0 | 1\n",
    "\n",
    "def PE(data):\n",
    "    '''On input data, return the P(E) (expected agreement).'''\n",
    "    relevant = 0\n",
    "    nonrelevant = 0\n",
    "    # Iterate over the data\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            \n",
    "            # Top up the relevant documents by one if 1 is encountered\n",
    "            if j == 1:\n",
    "                relevant += 1\n",
    "            # Top up the nonrelevant documents by one if 0 is encountered\n",
    "            if j == 0:\n",
    "                nonrelevant += 1\n",
    "\n",
    "    # Calculates the total of inspected documents for the judges combined\n",
    "    total = len(data)*2\n",
    "\n",
    "    # Calculates the pooled marginals\n",
    "    rel = relevant/total\n",
    "    nonrel = nonrelevant/total\n",
    "\n",
    "    # Calculates the P(E)\n",
    "    P_E = nonrel**2 + rel **2    \n",
    "    return    P_E \n",
    "\n",
    "\n",
    "def kappa(data, P_E):\n",
    "    agree = 0\n",
    "    for i in data:\n",
    "        temp = None\n",
    "        for j in i:\n",
    "            if temp == j:\n",
    "                agree += 1\n",
    "            temp = j\n",
    "    P_A = agree / len(data)\n",
    "    if P_E == 1:\n",
    "        kappa = 1\n",
    "    else:\n",
    "        kappa = (P_A - P_E)/(1 - P_E)   \n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AveragePrecision(ranked_list_of_results, list_of_relevant_objects):\n",
    "    total = len(list_of_relevant_objects)\n",
    "    sumPk = 0\n",
    "    rank = 0\n",
    "    relevant = 0\n",
    "    for result in ranked_list_of_results:\n",
    "        rank += 1\n",
    "        if result in list_of_relevant_objects:\n",
    "            relevant += 1\n",
    "            sumPk += relevant/rank\n",
    "            \n",
    "    aprecision = sumPk/total\n",
    "    return aprecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_result_list = list(range(20))\n",
    "relevant_objects_cos = [0,1,2,3,4,5,6,7,8,10,11,12,19]\n",
    "relevant_objects_kmeans = [0,1,4,5,6,7,8,9,10,11,15,16,17,18]\n",
    "print('Average Precision of cosine sim: ', AveragePrecision(ranked_result_list,relevant_objects_cos))\n",
    "print('Average Precision of K-Means: ', AveragePrecision(ranked_result_list,relevant_objects_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-Based Collaborative Filtering Algorithm <a name='itembasedcollab'></a>\n",
    "\n",
    "[Top](#Top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Stats for Collaborative Based <a name='basicdatacollab'></a>\n",
    "\n",
    "[Top](#Top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_reviews = df_reviews.reset_index()\n",
    "\n",
    "# count reviews per user and reviews per business\n",
    "reviews_per_user = df_reviews.groupby(\"user_id\")[\"review_id\"].count().sort_values(ascending=False).to_frame()\n",
    "reviews_per_business = df_reviews.groupby(\"business_id\")[\"review_id\"].count().sort_values(ascending=False).to_frame()\n",
    "\n",
    "\n",
    "# plot both review counts\n",
    "user_plot = reviews_per_user.plot()\n",
    "business_plot = reviews_per_business.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "userplot --> the distribution of the reviews over the users (long-tail): considering the fact that there are more\n",
    "reviews on business than users (one user can rate more than 1 business), it is probably better to choose\n",
    "item-based CF than user-based CF. \n",
    "\n",
    "\"\"\"\"\"\n",
    "users_series = df_users['review_count'].sort_values(ascending=False).drop_duplicates(keep='first')\n",
    "\n",
    "display(users_series)\n",
    "userplot = users_series.plot()\n",
    "\n",
    "userplot.set(xlabel='user_id', ylabel='review_count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation <a name='implementation'></a>\n",
    "\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deel 1: Mesa <a name='mesa'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df_reviews[:5000]\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_ratings(ratings):\n",
    "    pivottable = ratings.pivot_table(index=\"business_id\", columns=\"user_id\", values=\"stars\").fillna(0)\n",
    "    return pivottable\n",
    "\n",
    "utility_matrix = pivot_ratings(reviews)\n",
    "display(utility_matrix.loc['mRUVMJkUGxrByzMQ2MuOpA'][utility_matrix.loc['mRUVMJkUGxrByzMQ2MuOpA'].notna()])\n",
    "display(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(matrix, id1, id2):\n",
    "    selected_features = matrix.loc[id1].notna() & matrix.loc[id2].notna()\n",
    "    \n",
    "    # if no matching features, return 'not a number' (NaN)\n",
    "    if not selected_features.any():\n",
    "        return np.nan\n",
    "    \n",
    "    # get the features from the matrix\n",
    "    features1 = matrix.loc[id1][selected_features]\n",
    "    features2 = matrix.loc[id2][selected_features]\n",
    "\n",
    "    #if sum(features1)==0. or sum(features2)==0.:\n",
    "        #return 1.\n",
    "    \n",
    "    if id1 == id2:\n",
    "        return 1.\n",
    "    \n",
    "    sim = (np.sqrt(sum(features1**2))*np.sqrt(sum(features2**2)))\n",
    "    \n",
    "    if sim == 0.:\n",
    "        return np.nan\n",
    "    \n",
    "    return sum(features1*features2)/sim\n",
    "    \n",
    "def create_similarity_matrix_cosine(matrix):\n",
    "    similarity_matrix = pd.DataFrame(0, index=matrix.index, columns=matrix.index, dtype=float)\n",
    "    # Iter of rows & columns, cause I couldnt get apply/-map to work\n",
    "    for index, row in similarity_matrix.iteritems():\n",
    "        for i in row.index:\n",
    "            similarity_matrix.at[index,i] = cosine_distance(matrix,index,i)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "#display(create_similarity_matrix_euclid(matrix))\n",
    "    \n",
    "#similarity = create_similarity_matrix_cosine(utility_matrix)\n",
    "#display(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center_columns(matrix):\n",
    "#     return matrix.apply(lambda x: x - x.mean())\n",
    "\n",
    "# centered_utility_matrix = mean_center_columns(utility_matrix)\n",
    "# display(centered_utility_matrix)\n",
    "\n",
    "# similarity = create_similarity_matrix_cosine(centered_utility_matrix)\n",
    "# display(similarity)\n",
    "\n",
    "    mean_centered = matrix.copy()\n",
    "    for col in list(matrix.columns):\n",
    "        mean_centered[col] -= matrix[col].mean()\n",
    "        \n",
    "    return mean_centered\n",
    "\n",
    "\n",
    "centered_utility_matrix = mean_center_columns(utility_matrix)\n",
    "display(centered_utility_matrix)\n",
    "\n",
    "\n",
    "similarity = create_similarity_matrix_cosine(centered_utility_matrix)\n",
    "display(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.to_pickle('./similarity_pickle.pkl')\n",
    "similarity_matrix_mesa = pd.read_pickle('./similarity_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST neighborhood op eentje\n",
    "\n",
    "#def select_neighborhood(similarities, ratings, k):\n",
    "#    \"\"\" selects all items with similarity > 0  \"\"\"\n",
    "#    moviesNotWatched = ratings[ratings.isna()].index\n",
    "    \n",
    "#    similarities = similarities.where(lambda x : x > 0.00).dropna()\n",
    "#    similarities = similarities.drop(labels=moviesNotWatched,errors='ignore')\n",
    "    \n",
    "#    return similarities.sort_values(ascending=False)[:k]\n",
    "\n",
    "def select_neighborhood(similarities, ratings, k):\n",
    "    \"\"\" selects all items with similarity > 0  \"\"\"\n",
    "    # drop all non watched movies\n",
    "    \n",
    "    for key,v in ratings.iteritems():\n",
    "        if np.isnan(v):\n",
    "            similarities = similarities.drop(labels=key)\n",
    "    \n",
    "    for key,v in similarities.iteritems():\n",
    "        if not v > 0.:\n",
    "            similarities = similarities.drop(labels=key)\n",
    "            \n",
    "    similarities = similarities.sort_values(ascending=False)\n",
    "    \n",
    "    return similarities[:k]\n",
    "\n",
    "neighborhood = select_neighborhood(similarity[\"mRUVMJkUGxrByzMQ2MuOpA\"], utility_matrix[\"6G6_qNcvzRgAQdr2AWvkKw\"], 10)\n",
    "display(neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "def weighted_mean(neighborhood, ratings):  \n",
    "    upper = 0.\n",
    "    bottom = 0.\n",
    "    \n",
    "    for index in neighborhood.index:\n",
    "        similarity = neighborhood.get(index)\n",
    "        upper += ratings.get(index) * similarity\n",
    "        bottom += similarity\n",
    "    \n",
    "    if bottom == 0.:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return upper / bottom\n",
    "\n",
    "neighborhood1 = select_neighborhood(similarity[\"mRUVMJkUGxrByzMQ2MuOpA\"], utility_matrix[\"6G6_qNcvzRgAQdr2AWvkKw\"], 10)\n",
    "prediction1 = weighted_mean(neighborhood1, utility_matrix[\"6G6_qNcvzRgAQdr2AWvkKw\"])\n",
    "\n",
    "\n",
    "print(f\"User 6G6_qNcvzRgAQdr2AWvkKw predicted rating for business mRUVMJkUGxrByzMQ2MuOpA is {prediction1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_reviews[:500][[\"business_id\", \"user_id\", \"stars\"]].copy()\n",
    "\n",
    "def predict_ratings_item_based(similarity, utility, user_item_pairs):\n",
    "    ratings_test_c = user_item_pairs.copy()\n",
    "    ratings_test_c['predicted rating'] = 0.\n",
    "    for index,row in user_item_pairs.iterrows():\n",
    "        neighborhood = select_neighborhood(similarity[row['business_id']],utility[row['user_id']],1000)\n",
    "        ratings_test_c.at[index,'predicted rating'] = weighted_mean(neighborhood,utility[row['user_id']])\n",
    "    return ratings_test_c\n",
    "\n",
    "predicted_item_based = predict_ratings_item_based(similarity, utility_matrix, test_data)\n",
    "# display(predicted_item_based.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_reviews[:500][[\"business_id\", \"user_id\", \"stars\"]].copy()\n",
    "\n",
    "PR = predict_ratings_item_based(similarity, utility_matrix, test_data)\n",
    "# display(PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Predicted Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predicted_ratings):\n",
    "    return sum((predicted_ratings['stars']-predicted_ratings['predicted rating'])**2)/len(predicted_ratings)\n",
    "    \n",
    "\n",
    "mse_item_based = mse(PR)\n",
    "print(mse_item_based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_random = 0\n",
    "predicted_random = test_data.copy()[['user_id', 'business_id', 'stars']]\n",
    "predicted_random['predicted rating'] = 4.5 * np.random.random_sample((len(predicted_random),)) + 0.5\n",
    "\n",
    "display(predicted_random.head())\n",
    "\n",
    "mse_random = mse(predicted_random)\n",
    "\n",
    "print(f\"mse for item based prediction: {mse_item_based:.2f}\")\n",
    "print(f\"mse for random prediction: {mse_random:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deel 2: Henderson <a name='henderson'></a>\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Matrix & Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df_reviews[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_ratings(ratings):\n",
    "    pivottable = ratings.pivot_table(index=\"business_id\", columns=\"user_id\", values=\"stars\").fillna(0)\n",
    "    return pivottable\n",
    "\n",
    "utility_matrix = pivot_ratings(reviews)\n",
    "display(utility_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(matrix, id1, id2):\n",
    "    \n",
    "    selected_features = matrix.loc[id1].notna() & matrix.loc[id2].notna()\n",
    "    features1 = matrix.loc[id1][selected_features]\n",
    "    features2 = matrix.loc[id2][selected_features]\n",
    "    \n",
    "    if id1 == id2:\n",
    "        return 1\n",
    "    \n",
    "    elif selected_features.any() == False:\n",
    "        return np.nan\n",
    "                                                                                                \n",
    "    if (features1==0).all() and (features2==0).all() == True:\n",
    "         return np.nan\n",
    "    elif (features1==0).all() or (features2==0).all() == True:\n",
    "         return np.nan\n",
    "                                                   \n",
    "    else:\n",
    "         \n",
    "        teller = sum(features1*features2)\n",
    "        noemer = np.sqrt(sum(features1**2)) * np.sqrt(sum(features2**2))\n",
    "        cos = teller / noemer   \n",
    "        \n",
    "        return cos \n",
    "\n",
    "\n",
    "def create_similarity_matrix_cosine(matrix):\n",
    "    \n",
    "    cosine_similarity_matrix = pd.DataFrame(0, index=matrix.index, columns=matrix.index, dtype=float)\n",
    "    \n",
    "    for i1 in matrix.index.values:\n",
    "        for i2 in matrix.index.values:\n",
    "            similarity = cosine_similarity(matrix, i1, i2)\n",
    "            cosine_similarity_matrix[i1][i2] = similarity\n",
    "    \n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "\n",
    "#similarity = create_similarity_matrix_cosine(utility_matrix)\n",
    "#display(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center_columns(matrix):\n",
    "    result = matrix.copy()\n",
    "    \n",
    "    for i in result.columns.values:\n",
    "        m = result[i].mean()\n",
    "        for j in result.index.values:\n",
    "            result[i][j] = result[i][j] - m\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "centered_utility_matrix = mean_center_columns(utility_matrix)\n",
    "display(centered_utility_matrix)\n",
    "\n",
    "\n",
    "similarity = create_similarity_matrix_cosine(centered_utility_matrix)\n",
    "display(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.to_pickle('./similarity_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = pd.read_pickle('./similarity_pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_neighborhood(similarities, ratings, k):\n",
    "    \"\"\" selects all items with similarity > 0  \"\"\"\n",
    "   \n",
    "    results = {}\n",
    "    \n",
    "    for i in similarities.index.values:\n",
    "        if similarities[i]>0.000 and not np.isnan(ratings[i]):\n",
    "            print(similarities[i])\n",
    "            results[i] = similarities[i]\n",
    "    \n",
    "    df = pd.Series(results)\n",
    "        \n",
    "    \n",
    "    return df.sort_values(ascending=False)[:k]  \n",
    "        \n",
    "    \n",
    "#print(similarity[\"-3n__pVgU99k4jaSANVFgw\"], utility_matrix[\"-InhDRRVG7wrwsgAUvN4Qw\"])\n",
    "neighborhood = select_neighborhood(similarity[\"znRorbwFubHZaACq8qj2Rg\"], utility_matrix[\"zyFu57CLm1q752bkG9OjXQ\"], 10)\n",
    "display(neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_copy = reviews.copy()\n",
    "training_set = reviews_copy.sample(frac=0.80, random_state=0)[[\"business_id\", \"user_id\", \"stars\"]]\n",
    "test_set = reviews_copy.drop(training_set.index)[[\"business_id\", \"user_id\", \"stars\"]]\n",
    "\n",
    "display(training_set)\n",
    "display(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voorspelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(neighborhood, ratings):  \n",
    "    \n",
    "    a = 0.\n",
    "    b = 0.\n",
    "    \n",
    "    for (i, m) in neighborhood.iteritems():\n",
    "        a += m * ratings[i]\n",
    "        b += m \n",
    "\n",
    "    print(ratings)   \n",
    "    \n",
    "    if b == 0.:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return a/b\n",
    "    \n",
    "neighborhood = select_neighborhood(similarity[\"znRorbwFubHZaACq8qj2Rg\"], utility_matrix[\"zyFu57CLm1q752bkG9OjXQ\"], 10)\n",
    "prediction = weighted_mean(neighborhood, utility_matrix[\"zyFu57CLm1q752bkG9OjXQ\"])\n",
    "print(f'User \"-0HhZbPBlB1YZx3BhAfaEA\" predicted rating for \"zyFu57CLm1q752bkG9OjXQ\" is {prediction:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings_item_based(similarity, utility, user_item_pairs):\n",
    "    ratings_test_c = user_item_pairs.copy()\n",
    "    ratings_test_c['predicted rating'] = 0.\n",
    "    for index,row in user_item_pairs.iterrows():\n",
    "        neighborhood = select_neighborhood(similarity[row['business_id']],utility[row['user_id']],1000)\n",
    "        ratings_test_c.at[index,'predicted rating'] = weighted_mean(neighborhood,utility[row['user_id']])\n",
    "    return ratings_test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_training = predict_ratings_item_based(similarity, utility_matrix, training_set)\n",
    "PR_test = predict_ratings_item_based(similarity, utility_matrix, test_set)\n",
    "\n",
    "#display(PR_training)\n",
    "#display(PR_test)\n",
    "#PR = predict_ratings_item_based(similarity, utility_matrix, test_set)\n",
    "#test_data = df_reviews[:500][['business_id','user_id','stars']].copy()\n",
    "#display(PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Predicted Ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predicted_ratings):\n",
    "    return sum((predicted_ratings['stars']-predicted_ratings['predicted rating'])**2)/len(predicted_ratings)\n",
    "\n",
    "mse_item_based_training = mse(PR_training)\n",
    "mse_item_based_test = mse(PR_test)\n",
    "\n",
    "display(mse_item_based_training)\n",
    "display(mse_item_based_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Random Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_random = 0\n",
    "predicted_random = test_set.copy()[['user_id', 'business_id', 'stars']]\n",
    "predicted_random['predicted rating'] = 4.5 * np.random.random_sample((len(predicted_random),)) + 0.5\n",
    "\n",
    "display(predicted_random.head())\n",
    "\n",
    "mse_random = mse(predicted_random)\n",
    "print(f'mse for item based prediction: {mse_item_based_test:.2f}')\n",
    "print(f'mse for random prediction: {mse_random:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame\n",
    "# Split data in training and test set\n",
    "\n",
    "reviews_copy = reviews.copy()\n",
    "training_set = reviews_copy.sample(frac=0.80, random_state=0)\n",
    "test_set = reviews_copy.drop(training_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommended(predictions, threshold):\n",
    "    return predictions[predictions['predicted rating']>=threshold]\n",
    "    \n",
    "\n",
    "def hidden(predictions, threshold):\n",
    "    return predictions[predictions['predicted rating']<threshold]\n",
    "    \n",
    "#predicted_item_based    \n",
    "treshold_recommended = 3.75\n",
    "recommended_items = recommended(PR_test, treshold_recommended)\n",
    "hidden_items = hidden(PR_test, treshold_recommended)\n",
    "\n",
    "print(f'Test items : {PR_test.shape[0]}')\n",
    "print(f'Recommended: {recommended_items.shape[0]}')\n",
    "print(f'Hidden     : {hidden_items.shape[0]}')\n",
    "display(recommended_items.head())\n",
    "display(hidden_items.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def used(predictions, threshold):\n",
    "    return predictions[predictions['stars']>=threshold]\n",
    "    \n",
    "\n",
    "def unused(predictions, threshold):\n",
    "    return predictions[predictions['stars']<threshold]\n",
    "    \n",
    "    \n",
    "treshold_used = 4.0\n",
    "used_items = used(PR_test, treshold_used)\n",
    "unused_items = unused(PR_test, treshold_used)\n",
    "\n",
    "print(f'Test items: {PR_test.shape[0]}')\n",
    "print(f'Used      : {used_items.shape[0]}')\n",
    "print(f'Unused    : {unused_items.shape[0]}')\n",
    "display(used_items.head())\n",
    "display(unused_items.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(recommended, hidden, used, unused): \n",
    "    TP = len(recommended.merge(used)) \n",
    "    FP = len(recommended.merge(unused)) \n",
    "    TN = len(hidden.merge(unused))\n",
    "    FN = len(hidden.merge(used))\n",
    "    \n",
    "    return pd.DataFrame([[TP, FP], [FN, TN]], index=['recommended', 'hidden'], columns=['used', 'unused'])\n",
    "\n",
    "confusion_matrix = confusion(recommended_items, hidden_items, used_items, unused_items)\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(confusion_matrix):\n",
    "    return confusion_matrix.at['recommended','used']/(confusion_matrix.at['recommended','used']+confusion_matrix.at['recommended','unused'])\n",
    "\n",
    "precision_item_based = precision(confusion_matrix)\n",
    "\n",
    "print(f'precision for item based prediction: {precision_item_based:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(confusion_matrix):\n",
    "    return confusion_matrix.at['recommended','used']/(confusion_matrix.at['recommended','used']+confusion_matrix.at['hidden','used'])\n",
    "\n",
    "recall_item_based = recall(confusion_matrix)\n",
    "\n",
    "print(f'recall for item based prediction: {recall_item_based:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold_recommended = 3.75\n",
    "treshold_used = 4.0\n",
    "\n",
    "recommended_item_based = recommended(PR_test, treshold_recommended)\n",
    "hidden_item_based = hidden(PR_test, treshold_recommended)\n",
    "used_item_based = used(PR_test, treshold_used)\n",
    "unused_item_based = unused(PR_test, treshold_used)\n",
    "\n",
    "recommended_random = recommended(predicted_random, treshold_recommended)\n",
    "hidden_random = hidden(predicted_random, treshold_recommended)\n",
    "used_random = used(predicted_random, treshold_used)\n",
    "unused_random = unused(predicted_random, treshold_used)\n",
    "\n",
    "confusion_item_based = confusion(recommended_item_based,hidden_item_based,used_item_based,unused_item_based)\n",
    "confusion_random = confusion(recommended_random, hidden_random,used_random,unused_random)\n",
    "\n",
    "precision_random = precision(confusion_random)\n",
    "recall_random = recall(confusion_random)\n",
    "\n",
    "print('           | precision | recall')\n",
    "print(f'item based |      {precision_item_based:.2f} |   {recall_item_based:.2f}')\n",
    "print(f'random     |      {precision_random:.2f} |   {recall_random:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold_used = 3.75\n",
    "treshold_recommended = [2.5,3.,3.5,4.,4.5]\n",
    "\n",
    "recommended_item_based = [recommended(PR_test, thresh) for thresh in treshold_recommended]\n",
    "hidden_item_based = [hidden(PR_test, thresh) for thresh in treshold_recommended]\n",
    "used_item_based = used(PR_test, treshold_used)\n",
    "unused_item_based = unused(PR_test, treshold_used)\n",
    "\n",
    "recommended_random = [recommended(predicted_random, thresh) for thresh in treshold_recommended]\n",
    "hidden_random = [hidden(predicted_random, thresh) for thresh in treshold_recommended]\n",
    "used_random = used(predicted_random, treshold_used)\n",
    "unused_random = unused(predicted_random, treshold_used)\n",
    "\n",
    "confusion_item_based = [confusion(recommended_item_based[x],hidden_item_based[x],used_item_based,unused_item_based) for x in range(len(treshold_recommended))]\n",
    "confusion_random = [confusion(recommended_random[x],hidden_random[x],used_random,unused_random) for x in range(len(treshold_recommended))]\n",
    "\n",
    "precision_item_based = [precision(confusion_item_based[x]) for x in range(len(treshold_recommended))]\n",
    "recall_item_based = [recall(confusion_item_based[x]) for x in range(len(treshold_recommended))]\n",
    "\n",
    "precision_random = [precision(confusion_random[x]) for x in range(len(treshold_recommended))]\n",
    "recall_random = [recall(confusion_random[x]) for x in range(len(treshold_recommended))]\n",
    "\n",
    "plt.plot(recall_item_based, precision_item_based)\n",
    "for r, p, t in zip(recall_item_based, precision_item_based, treshold_recommended):\n",
    "    plt.text(r, p, t)\n",
    "\n",
    "plt.plot(recall_random, precision_random)\n",
    "for r, p, t in zip(recall_random, precision_random, treshold_recommended):\n",
    "    plt.text(r, p, t)\n",
    "    \n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.3, 1.0)\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "\n",
    "\n",
    "plt.legend(['item_based','random'], loc = 'lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GroepsOpdracht_CI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
